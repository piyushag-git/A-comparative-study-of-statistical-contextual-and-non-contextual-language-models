### A comparative study of statistical, contextual and non-contextual language models.

Solving question answering tasks using language models is a very crucial problem to research on. While it can seem easy on the surface because humans can seemingly solve it very efficiently, there are a number of challenges when it comes to machines. A question answering task assumes a certain level of contextual understanding which is one of the fundamental drawbacks of language models. Prior to advancements in deep learning, statistical and human-annotated language models were used in an attempt to resolve the issue of interpreting context using the frequency of words, probability distributions, and hierarchical similarity. Currently, word embeddings are the focus of substantial research. Word embeddings are n-dimensional vector representations of words in a corpus. The idea behind word embeddings is that similar words occupy similar vector spaces. To do this effectively, a word embedding model needs to train on huge datasets. There are non-contextual word embedding sets such as GloVe and word2vec and transformer-based contextual embeddings such as BERT, GPT, etc. This project focuses on comparing the results of statistical (Markov assumption-based n-gram models), non-contextual (Word2Vec and GloVe), and contextual embeddings (BERT) on the Microsoft Sentence Completion Challenge dataset.
